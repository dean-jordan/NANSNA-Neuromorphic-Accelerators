{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "import snntorch\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "from snntorch import backprop\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "from snntorch import utils\n",
    "\n",
    "from torch.utils.data import dataloader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReluActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeakyReluActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x, negative_slope=0.1):\n",
    "        if x > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return x * negative_slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReluActivation, self).__init__()\n",
    "\n",
    "    def forward(self):\n",
    "        x = torch.maximum(x, 0.0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigmoidActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 1/(1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SoftmaxActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.exp(x) / sum(torch.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
