@article{Hu2023,
   abstract = {Spiking neural networks (SNNs) have received significant attention for their biological plausibility. SNNs theoretically have at least the same computational power as traditional artificial neural networks (ANNs). They possess the potential of achieving energy-efficient machine intelligence while keeping comparable performance to ANNs. However, it is still a big challenge to train a very deep SNN. In this brief, we propose an efficient approach to build deep SNNs. Residual network (ResNet) is considered a state-of-the-art and fundamental model among convolutional neural networks (CNNs). We employ the idea of converting a trained ResNet to a network of spiking neurons named spiking ResNet (S-ResNet). We propose a residual conversion model that appropriately scales continuous-valued activations in ANNs to match the firing rates in SNNs and a compensation mechanism to reduce the error caused by discretization. Experimental results demonstrate that our proposed method achieves state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet 2012 with low latency. This work is the first time to build an asynchronous SNN deeper than 100 layers, with comparable performance to its original ANN.},
   author = {Yangfan Hu and Huajin Tang and Gang Pan},
   doi = {10.1109/TNNLS.2021.3119238},
   issn = {21622388},
   issue = {8},
   journal = {IEEE Transactions on Neural Networks and Learning Systems},
   title = {Spiking Deep Residual Networks},
   volume = {34},
   year = {2023},
}
@article{Kim2021,
   abstract = {Spiking Neural Networks (SNNs) have recently emerged as an alternative to deep learning owing to sparse, asynchronous and binary event (or spike) driven processing, that can yield huge energy efficiency benefits on neuromorphic hardware. However, SNNs convey temporally-varying spike activation through time that is likely to induce a large variation of forward activation and backward gradients, resulting in unstable training. To address this training issue in SNNs, we revisit Batch Normalization (BN) and propose a temporal Batch Normalization Through Time (BNTT) technique. Different from previous BN techniques with SNNs, we find that varying the BN parameters at every time-step allows the model to learn the time-varying input distribution better. Specifically, our proposed BNTT decouples the parameters in a BNTT layer along the time axis to capture the temporal dynamics of spikes. We demonstrate BNTT on CIFAR-10, CIFAR-100, Tiny-ImageNet, event-driven DVS-CIFAR10 datasets, and Sequential MNIST and show near state-of-the-art performance. We conduct comprehensive analysis on the temporal characteristic of BNTT and showcase interesting benefits toward robustness against random and adversarial noise. Further, by monitoring the learnt parameters of BNTT, we find that we can do temporal early exit. That is, we can reduce the inference latency by ~5 − 20 time-steps from the original training latency. The code has been released at https://github.com/Intelligent-Computing-Lab-Yale/BNTT-Batch-Normalization-Through-Time.},
   author = {Youngeun Kim and Priyadarshini Panda},
   doi = {10.3389/fnins.2021.773954},
   issn = {1662453X},
   journal = {Frontiers in Neuroscience},
   title = {Revisiting Batch Normalization for Training Low-Latency Deep Spiking Neural Networks From Scratch},
   volume = {15},
   year = {2021},
}
@article{Bekolay2014,
   abstract = {Neuroscience currently lacks a comprehensive theory of how cognitive processes can be implemented in a biological substrate. The Neural Engineering Framework (NEF) proposes one such theory, but has not yet gathered significant empirical support, partly due to the technical challenge of building and simulating large-scale models with the NEF. Nengo is a software tool that can be used to build and simulate large-scale models based on the NEF; currently, it is the primary resource for both teaching how the NEF is used, and for doing research that generates specific NEF models to explain experimental data. Nengo 1.4, which was implemented in Java, was used to create Spaun, the world's largest functional brain model (Eliasmith et al., 2012). Simulating Spaun highlighted limitations in Nengo 1.4's ability to support model construction with simple syntax, to simulate large models quickly, and to collect large amounts of data for subsequent analysis. This paper describes Nengo 2.0, which is implemented in Python and overcomes these limitations. It uses simple and extendable syntax, simulates a benchmark model on the scale of Spaun 50 times faster than Nengo 1.4, and has a flexible mechanism for collecting simulation results. © 2014 Bekolay, Bergstra, Hunsberger, DeWolf, Stewart, Rasmussen, Choo, Voelker and Eliasmith.},
   author = {Trevor Bekolay and James Bergstra and Eric Hunsberger and Travis DeWolf and Terrence C. Stewart and Daniel Rasmussen and Xuan Choo and Aaron Russell Voelker and Chris Eliasmith},
   doi = {10.3389/fninf.2013.00048},
   issn = {16625196},
   issue = {JAN},
   journal = {Frontiers in Neuroinformatics},
   keywords = {Control theory,Nengo,Neural engineering framework,Neuroscience,Python,Simulation,Theoretical neuroscience},
   month = {1},
   title = {Nengo: A Python tool for building large-scale functional brain models},
   volume = {7},
   year = {2014},
}
@article{Zhu2023,
   abstract = {As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language. We achieve this by modifying the transformer block to replace multi-head self attention to reduce quadratic computational complexity O(N^2) to linear complexity O(N) with increasing sequence length. Input tokens are instead streamed in sequentially to our attention mechanism (as with typical SNNs). Our preliminary experiments show that SpikeGPT remains competitive with non-spiking models on tested benchmarks, while maintaining 20x fewer operations when processed on neuromorphic hardware that can leverage sparse, event-driven activations. Our code implementation is available at https://github.com/ridgerchu/SpikeGPT.},
   author = {Rui-Jie Zhu and Qihang Zhao and Guoqi Li and Jason K. Eshraghian},
   month = {2},
   title = {SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks},
   url = {http://arxiv.org/abs/2302.13939},
   year = {2023},
}
@article{Orchard2015,
   abstract = {Creating datasets for Neuromorphic Vision is a challenging task. A lack of available recordings from Neuromorphic Vision sensors means that data must typically be recorded specifically for dataset creation rather than collecting and labeling existing data. The task is further complicated by a desire to simultaneously provide traditional frame-based recordings to allow for direct comparison with traditional Computer Vision algorithms. Here we propose a method for converting existing Computer Vision static image datasets into Neuromorphic Vision datasets using an actuated pan-tilt camera platform. Moving the sensor rather than the scene or image is a more biologically realistic approach to sensing and eliminates timing artifacts introduced by monitor updates when simulating motion on a computer monitor. We present conversion of two popular image datasets (MNIST and Caltech101) which have played important roles in the development of Computer Vision, and we provide performance metrics on these datasets using spike-based recognition algorithms. This work contributes datasets for future use in the field, as well as results from spike-based algorithms against which future works can compare. Furthermore, by converting datasets already popular in Computer Vision, we enable more direct comparison with frame-based approaches.},
   author = {Garrick Orchard and Ajinkya Jayawant and Gregory K. Cohen and Nitish Thakor},
   doi = {10.3389/fnins.2015.00437},
   issn = {1662453X},
   issue = {NOV},
   journal = {Frontiers in Neuroscience},
   keywords = {Benchmarking,Computer vision,Datasets,Neuromorphic Vision,Sensory processing},
   publisher = {Frontiers Research Foundation},
   title = {Converting static image datasets to spiking neuromorphic datasets using saccades},
   volume = {9},
   year = {2015},
}
@article{Whitaker2023,
   abstract = {Neural network ensembles have been effectively used to improve generalization by combining the predictions of multiple independently trained models. However, the growing scale and complexity of deep neural networks have led to these methods becoming prohibitively expensive and time consuming to implement. Low-cost ensemble methods have become increasingly important as they can alleviate the need to train multiple models from scratch while retaining the generalization benefits that traditional ensemble learning methods afford. This dissertation introduces and formalizes a low-cost framework for constructing Subnetwork Ensembles, where a collection of child networks are formed by sampling, perturbing, and optimizing subnetworks from a trained parent model. We explore several distinct methodologies for generating child networks and we evaluate their efficacy through a variety of ablation studies and established benchmarks. Our findings reveal that this approach can greatly improve training efficiency, parametric utilization, and generalization performance while minimizing computational cost. Subnetwork Ensembles offer a compelling framework for exploring how we can build better systems by leveraging the unrealized potential of deep neural networks.},
   author = {Tim Whitaker},
   month = {11},
   title = {Neural Subnetwork Ensembles},
   url = {http://arxiv.org/abs/2311.14101},
   year = {2023},
}
@inproceedings{Dold2022,
   abstract = {Knowledge graphs are an expressive and widely used data structure due to their ability to integrate data from different domains in a sensible and machine-readable way. Thus, they can be used to model a variety of systems such as molecules and social networks. However, it still remains an open question how symbolic reasoning could be realized in spiking systems and, therefore, how spiking neural networks could be applied to such graph data. Here, we extend previous work on spike-based graph algorithms by demonstrating how symbolic and multi-relational information can be encoded using spiking neurons, allowing reasoning over symbolic structures like knowledge graphs with spiking neural networks. The introduced framework is enabled by combining the graph embedding paradigm and the recent progress in training spiking neural networks using error backpropagation. The presented methods are applicable to a variety of spiking neuron models and can be trained end-to-end in combination with other differentiable network architectures, which we demonstrate by implementing a spiking relational graph neural network.},
   author = {Dominik Dold and Josep Soler Garrido and Victor Caceres Chian and Marcel Hildebrandt and Thomas Runkler},
   doi = {10.1145/3546790.3546824},
   booktitle = {ACM International Conference Proceeding Series},
   title = {Neuro-symbolic computing with spiking neural networks},
   year = {2022},
}
@article{Eshraghian2023,
   abstract = {The brain is the perfect place to look for inspiration to develop more efficient neural networks. The inner workings of our synapses and neurons provide a glimpse at what the future of deep learning might look like. This article serves as a tutorial and perspective showing how to apply the lessons learned from several decades of research in deep learning, gradient descent, backpropagation, and neuroscience to biologically plausible spiking neural networks (SNNs). We also explore the delicate interplay between encoding data as spikes and the learning process; the challenges and solutions of applying gradient-based learning to SNNs; the subtle link between temporal backpropagation and spike timing-dependent plasticity; and how deep learning might move toward biologically plausible online learning. Some ideas are well accepted and commonly used among the neuromorphic engineering community, while others are presented or justified for the first time here. A series of companion interactive tutorials complementary to this article using our Python package, snnTorch, are also made available: https://snntorch.readthedocs.io/en/latest/tutorials/index.html.},
   author = {Jason K. Eshraghian and Max Ward and Emre O. Neftci and Xinxin Wang and Gregor Lenz and Girish Dwivedi and Mohammed Bennamoun and Doo Seok Jeong and Wei D. Lu},
   doi = {10.1109/JPROC.2023.3308088},
   issn = {15582256},
   issue = {9},
   journal = {Proceedings of the IEEE},
   title = {Training Spiking Neural Networks Using Lessons from Deep Learning},
   volume = {111},
   year = {2023},
}
@article{Zhou2022,
   abstract = {We consider two biologically plausible structures, the Spiking Neural Network (SNN) and the self-attention mechanism. The former offers an energy-efficient and event-driven paradigm for deep learning, while the latter has the ability to capture feature dependencies, enabling Transformer to achieve good performance. It is intuitively promising to explore the marriage between them. In this paper, we consider leveraging both self-attention capability and biological properties of SNNs, and propose a novel Spiking Self Attention (SSA) as well as a powerful framework, named Spiking Transformer (Spikformer). The SSA mechanism in Spikformer models the sparse visual feature by using spike-form Query, Key, and Value without softmax. Since its computation is sparse and avoids multiplication, SSA is efficient and has low computational energy consumption. It is shown that Spikformer with SSA can outperform the state-of-the-art SNNs-like frameworks in image classification on both neuromorphic and static datasets. Spikformer (66.3M parameters) with comparable size to SEW-ResNet-152 (60.2M,69.26%) can achieve 74.81% top1 accuracy on ImageNet using 4 time steps, which is the state-of-the-art in directly trained SNNs models.},
   author = {Zhaokun Zhou and Yuesheng Zhu and Chao He and Yaowei Wang and Shuicheng Yan and Yonghong Tian and Li Yuan},
   month = {9},
   title = {Spikformer: When Spiking Neural Network Meets Transformer},
   url = {http://arxiv.org/abs/2209.15425},
   year = {2022},
}
@article{Wanjura2024,
   abstract = {The increasing size of neural networks for deep learning applications and their energy consumption create a need for alternative neuromorphic approaches, for example, using optics. Current proposals and implementations rely on physical nonlinearities or optoelectronic conversion to realize the required nonlinear activation function. However, there are considerable challenges with these approaches related to power levels, control, energy efficiency and delays. Here we present a scheme for a neuromorphic system that relies on linear wave scattering and yet achieves nonlinear processing with high expressivity. The key idea is to encode the input in physical parameters that affect the scattering processes. Moreover, we show that gradients needed for training can be directly measured in scattering experiments. We propose an implementation using integrated photonics based on racetrack resonators, which achieves high connectivity with a minimal number of waveguide crossings. Our work introduces an easily implementable approach to neuromorphic computing that can be widely applied in existing state-of-the-art scalable platforms, such as optics, microwave and electrical circuits.},
   author = {Clara C. Wanjura and Florian Marquardt},
   doi = {10.1038/s41567-024-02534-9},
   issn = {17452481},
   journal = {Nature Physics},
   publisher = {Nature Research},
   title = {Fully nonlinear neuromorphic computing with linear wave scattering},
   year = {2024},
}
@inproceedings{Vishwa2020,
   abstract = {Neuromorphic computing is a budding avenue though it has been known since the 80's. The extensive research and development in the field of artificial intelligence particularly in the last decade is tremendous. The growth of artificial intelligence is expected to grow exponential in the years to come. Technologies like machine learning and IoT has made possible for many fields from industrial automation to business model prediction very affordable and far less complex. With growing digital devices, the number of devices connected to the cloud and in a network is doubling and in some cases are tripling in some ventures. Technologies like drones, autonomous cars, smart healthcare, smart cities and many other are moving towards more and more data and connected devices to the cloud. The present hardware system is at the verge of giving away as the data generation rate and processing volumes of the same is becoming a challenge. The hardware of today, though are advance are simply not adequate to support the expansion rate of growth of artificial intelligence in all fields. Increased devices result in increase data, increased processing raising challenges for current storage devices and processing devices. Neuromorphic chips, which promise to overcome this challenge, are currently being researched extensively by many computer giants who fear the future incompetency of hardware of which IBM is a major player. Ground breaking research in the field of memristor and artificial synapse have paved the way for neuromorphic chips which are expected to revolutionized the field for the better. This paper deals with the current research, physical and technical limitations and future scope of neuromorphic chips. The significance of memristor and artificial synapse towards neuromorphic computing is also dealt in detail.},
   author = {R. Vishwa and R. Karthikeyan and R. Rohith and A. Sabaresh},
   doi = {10.1088/1757-899X/912/6/062029},
   issn = {1757899X},
   issue = {6},
   booktitle = {IOP Conference Series: Materials Science and Engineering},
   title = {Current Research and Future Prospects of Neuromorphic Computing in Artificial Intelligence},
   volume = {912},
   year = {2020},
}
@misc{Ivanov2022,
   abstract = {Modern artificial intelligence (AI) systems, based on von Neumann architecture and classical neural networks, have a number of fundamental limitations in comparison with the mammalian brain. In this article we discuss these limitations and ways to mitigate them. Next, we present an overview of currently available neuromorphic AI projects in which these limitations are overcome by bringing some brain features into the functioning and organization of computing systems (TrueNorth, Loihi, Tianjic, SpiNNaker, BrainScaleS, NeuronFlow, DYNAP, Akida, Mythic). Also, we present the principle of classifying neuromorphic AI systems by the brain features they use: connectionism, parallelism, asynchrony, impulse nature of information transfer, on-device-learning, local learning, sparsity, analog, and in-memory computing. In addition to reviewing new architectural approaches used by neuromorphic devices based on existing silicon microelectronics technologies, we also discuss the prospects for using a new memristor element base. Examples of recent advances in the use of memristors in neuromorphic applications are also given.},
   author = {Dmitry Ivanov and Aleksandr Chezhegov and Mikhail Kiselev and Andrey Grunin and Denis Larionov},
   doi = {10.3389/fnins.2022.959626},
   issn = {1662453X},
   journal = {Frontiers in Neuroscience},
   title = {Neuromorphic artificial intelligence systems},
   volume = {16},
   year = {2022},
}
@inproceedings{Date2022,
   abstract = {Neuromorphic computers perform computations by emulating the human brain and are expected to be indispensable for energy-efficient computing in the future. They are primarily used in spiking neural network-based machine learning applications. However, neuromorphic computers are unable to preprocess data for these applications. Currently, data is preprocessed on a CPU or a GPU-this incurs a significant cost of transferring data from the CPU/GPU to the neuromorphic processor and vice versa. This cost can be avoided if preprocessing is done on the neuromorphic processor. To efficiently preprocess data on a neuromorphic processor, we first need an efficient mechanism for encoding data that can lend itself to all general-purpose preprocessing operations. Current encoding approaches have limited applicability and may not be suitable for all preprocessing operations. In this paper, we present the virtual neuron as a mechanism for encoding integers and rational numbers on neuromorphic processors. We evaluate the performance of the virtual neuron on physical and simulated neuromorphic hardware and show that it can perform an addition operation using 23 nJ of energy on average using a mixed-signal, memristor-based neuromorphic processor. The virtual neuron encoding approach is the first step in preprocessing data on a neuromorphic processor.},
   author = {Prasanna Date and Shruti Kulkarni and Aaron Young and Catherine Schuman and Thomas Potok and Jeffrey S. Vetter},
   doi = {10.1109/ICRC57508.2022.00017},
   booktitle = {Proceedings - 2022 IEEE International Conference on Rebooting Computing, ICRC 2022},
   title = {Virtual Neuron: A Neuromorphic Approach for Encoding Numbers},
   year = {2022},
}
@article{,
   abstract = {<p>Neuromorphic computing, a brain-inspired non-Von Neumann computing system, addresses the challenges posed by the Moore’s law memory wall phenomenon. It has the capability to enhance performance while maintaining power efficiency. Neuromorphic chip architecture requirements vary depending on the application and optimising it for large-scale applications remains a challenge. Neuromorphic chips are programmed using spiking neural networks which provide them with important properties such as parallelism, asynchronism, and on-device learning. Widely used spiking neuron models include the Hodgkin–Huxley Model, Izhikevich model, integrate-and-fire model, and spike response model. Hardware implementation platforms of the chip follow three approaches: analogue, digital, or a combination of both. Each platform can be implemented using various memory topologies which interconnect with the learning mechanism. Current neuromorphic computing systems typically use the unsupervised learning spike timing-dependent plasticity algorithms. However, algorithms such as voltage-dependent synaptic plasticity have the potential to enhance performance. This review summarises the potential neuromorphic chip architecture specifications and highlights which applications they are suitable for.</p>},
   author = {Seham Al Abdul Wahid and Arghavan Asad and Farah Mohammadi},
   doi = {10.3390/electronics13152963},
   issn = {2079-9292},
   issue = {15},
   journal = {Electronics},
   month = {7},
   pages = {2963},
   title = {A Survey on Neuromorphic Architectures for Running Artificial Intelligence Algorithms},
   volume = {13},
   url = {https://www.mdpi.com/2079-9292/13/15/2963},
   year = {2024},
}
@article{Kulkarni2021,
   abstract = {Artificial Intelligence is becoming ubiquitous in products and services that we use daily. Although the domain of AI has seen substantial improvements over recent years, its effectiveness is limited by the capabilities of current computing technology. Recently, there have been several architectural innovations for AI using emerging nanotechnology. These architectures implement mathematical computations of AI with circuits that utilize physical behavior of nanodevices purpose-built for such computations. This approach leads to a much greater efficiency vs. software algorithms running on von Neumann processors or CMOS architectures, which emulate the operations with transistor circuits. In this article, we provide a comprehensive survey of these architectural directions and categorize them based on their contributions. Furthermore, we discuss the potential offered by these directions with real-world examples. We also discuss major challenges and opportunities in this field.},
   author = {Sourabh Kulkarni and Sachin Bhat and Csaba Andras Moritz},
   doi = {10.1145/3445977},
   issn = {15504840},
   issue = {3},
   journal = {ACM Journal on Emerging Technologies in Computing Systems},
   title = {Architecting for artificial intelligence with emerging nanotechnology},
   volume = {17},
   year = {2021},
}
@article{Staudigl2022,
   abstract = {This work is a survey of neuromorphic computing-in-memory. Unlike existing surveys that focus on hardware or application-level perspectives, the authors elaborate on architectures, simulators, and security from a system point of view. At the end of the article, the authors highlight the challenges in neuromorphic computing-in-memory and raise a few open questions for future research in this field.},
   author = {Felix Staudigl and Farhad Merchant and Rainer Leupers},
   doi = {10.1109/MDAT.2021.3102013},
   issn = {21682364},
   issue = {2},
   journal = {IEEE Design and Test},
   title = {A Survey of Neuromorphic Computing-in-Memory: Architectures, Simulators, and Security},
   volume = {39},
   year = {2022},
}
@inproceedings{Telminov2022,
   abstract = {Extensive development of new neuromorphic element base - non-volatile memory elements based on new physical principles (ReRAM, FRAM etc.) is conducted. These memory elements are used to implement programmable synaptic weights in crossbar architecture, and enable neural network to conduct in-memory computations. However, the sneak currents and leakage currents are a serious limitation on the achievable dimensionality of rows and columns of the crossbar. The features of the implementation of neural networks on memristor crossbars are considered.},
   author = {Oleg Telminov and Eugeny Gornev},
   doi = {10.1109/DCNA56428.2022.9923302},
   booktitle = {Proceedings - 6th Scientific School "Dynamics of Complex Networks and their Applications", DCNA 2022},
   title = {Possibilities and Limitations of Memristor Crossbars for Neuromorphic Computing},
   year = {2022},
}
@article{Machado2023,
   abstract = {Neuromorphic computing offers a promising solution to overcome the von Neumann bottleneck, where the separation between the memory and the processor poses increasing limitations of latency and power consumption. For this purpose, a device with analog switching for weight update is necessary to implement neuromorphic applications. In the diversity of emerging devices postulated as synaptic elements in neural networks, RRAM emerges as a standout candidate for its ability to tune its resistance. The learning accuracy of a neural network is directly related to the linearity and symmetry of the weight update behavior of the synaptic element. However, it is challenging to obtain such a linear and symmetrical behavior with RRAM devices. Thus, extensive research is currently devoted at different levels, from material to device engineering, to improve the linearity and symmetry of the conductance update of RRAM devices. In this work, the experimental results based on different programming pulse conditions of RRAM devices are presented, considering both voltage and current pulses. Their suitability for application as analog RRAM-based synaptic devices for neuromorphic computing is analyzed by computing an asymmetric nonlinearity factor.},
   author = {Pau Machado and Salvador Manich and Álvaro Gómez-Pau and Rosa Rodríguez-Montañés and Mireia Bargalló González and Francesca Campabadal and Daniel Arumí},
   doi = {10.3390/electronics12234803},
   issn = {20799292},
   issue = {23},
   journal = {Electronics (Switzerland)},
   title = {Programming Techniques of Resistive Random-Access Memory Devices for Neuromorphic Computing},
   volume = {12},
   year = {2023},
}
@article{Hong2024,
   abstract = {Neuromorphic computing is an emerging research field that aims to develop new intelligent systems by integrating theories and technologies from multiple disciplines, such as neuroscience, deep learning and microelectronics. Various software frameworks have been developed for related fields, but an efficient framework dedicated to spike-based computing models and algorithms is lacking. In this work, we present a Python-based spiking neural network (SNN) simulation and training framework, named SPAIC, that aims to support brain-inspired model and algorithm research integrated with features from both deep learning and neuroscience. To integrate different methodologies from multiple disciplines and balance flexibility and efficiency, SPAIC is designed with a neuroscience-style frontend and a deep learning-based backend. Various types of examples are provided to demonstrate the wide usability of the framework, including neural circuit simulation, deep SNN learning and neuromorphic applications. As a user-friendly, flexible, and high-performance software tool, it will help accelerate the rapid growth and wide applicability of neuromorphic computing methodologies.},
   author = {Chaofei Hong and Mengwen Yuan and Mengxiao Zhang and Xiao Wang and Chengjun Zhang and Jiaxin Wang and Gang Pan and Huajin Tang},
   doi = {10.1109/MCI.2023.3327842},
   issn = {15566048},
   issue = {1},
   journal = {IEEE Computational Intelligence Magazine},
   title = {SPAIC: A Spike-Based Artificial Intelligence Computing Framework},
   volume = {19},
   year = {2024},
}
@inproceedings{Manna2023,
   abstract = {The Neuromorphic (NM) field has seen significant growth in recent years, especially in the development of Machine Learning (ML) applications. Developing effective learning systems for such applications requires extensive experimentation and simulation, which can be facilitated by using software frameworks that provide researchers with a set of ready-to-use tools. The NM technological landscape has witnessed the emergence of several new frameworks in addition to the existing libraries in neuroscience fields. This work reviews nine frameworks for developing Spiking Neural Networks (SNNs) that are specifically oriented towards data science applications. We emphasize the availability of spiking neuron models and learning rules to more easily direct decisions on the most suitable frameworks to carry out different types of research. Furthermore, we present an extension to the SpykeTorch framework that enables users to incorporate a broader range of neuron models in SNNs trained with Spike-Timing-Dependent Plasticity (STDP). The extended code is made available to the public, providing a valuable resource for researchers in this field.},
   author = {Davide L. Manna and Alex Vicente-Sola and Paul Kirkland and Trevor J. Bihl and Gaetano Di Caterina},
   doi = {10.1007/978-3-031-34204-2_20},
   issn = {18650937},
   booktitle = {Communications in Computer and Information Science},
   title = {Frameworks for SNNs: A Review of Data Science-Oriented Software and an Expansion of SpykeTorch},
   volume = {1826 CCIS},
   year = {2023},
}
@article{Stefanini2014,
   abstract = {Neuromorphic hardware offers an electronic substrate for the realization of asynchronous event-based sensory-motor systems and large-scale spiking neural network architectures. In order to characterize these systems, configure them, and carry out modeling experiments, it is often necessary to interface them to workstations. The software used for this purpose typically consists of a large monolithic block of code which is highly specific to the hardware setup used. While this approach can lead to highly integrated hardware/software systems, it hampers the development of modular and reconfigurable infrastructures thus preventing a rapid evolution of such systems. To alleviate this problem, we propose PyNCS, an open-source front-end for the definition of neural network models that is interfaced to the hardware through a set of Python Application Programming Interfaces (APIs). The design of PyNCS promotes modularity, portability and expandability and separates implementation from hardware description. The high-level front-end that comes with PyNCS includes tools to define neural network models as well as to create, monitor and analyze spiking data. Here we report the design philosophy behind the PyNCS framework and describe its implementation. We demonstrate its functionality with two representative case studies, one using an event-based neuromorphic vision sensor, and one using a set of multi-neuron devices for carrying out a cognitive decision-making task involving state-dependent computation. PyNCS, already applicable to a wide range of existing spike-based neuromorphic setups, will accelerate the development of hybrid software/hardware neuromorphic systems, thanks to its code flexibility. The code is open-source and available online at https://github.com/inincs/pyNCS.},
   author = {Fabio Stefanini and Emre O. Neftci and Sadique Sheik and Giacomo Indiveri},
   doi = {10.3389/fninf.2014.00073},
   journal = {Frontiers in Neuroinformatics},
   title = {PyNCS: a microkernel for high-level definition and configuration of neuromorphic electronic systems},
   volume = {8},
   year = {2014},
}
@article{Michaelis2020,
   abstract = {High-level frameworks for spiking neural networks are a key factor for fast prototyping and efficient development of complex algorithms. Such frameworks have emerged in the last years for traditional computers, but programming neuromorphic hardware is still a challenge. Often low level programming with knowledge about the hardware of the neuromorphic chip is required. The PeleNet framework aims to simplify reservoir computing for the neuromorphic hardware Loihi. It is build on top of the NxSDK from Intel and is written in Python. The framework manages weight matrices, parameters and probes. In particular, it provides an automatic and efficient distribution of networks over several cores and chips. With this, the user is not confronted with technical details and can concentrate on experiments.},
   author = {Carlo Michaelis},
   month = {11},
   title = {PeleNet: A Reservoir Computing Framework for Loihi},
   url = {http://arxiv.org/abs/2011.12338},
   year = {2020},
}
@article{Hazan2022,
   abstract = {Neuromorphic hardware designs realize neural principles in electronics to provide highperforming, energy-efficient frameworks for machine learning. Here, we propose a neuromorphic analog design for continuous real-time learning. Our hardware design realizes the underlying principles of the neural engineering framework (NEF). NEF brings forth a theoretical framework for the representation and transformation of mathematical constructs with spiking neurons, thus providing efficient means for neuromorphic machine learning and the design of intricate dynamical systems. Our analog circuit design implements the neuromorphic prescribed error sensitivity (PES) learning rule with OZ neurons. OZ is an analog implementation of a spiking neuron, which was shown to have complete correspondence with NEF across firing rates, encoding vectors, and intercepts. We demonstrate PES-based neuromorphic representation of mathematical constructs with varying neuron configurations, the transformation of mathematical constructs, and the construction of a dynamical system with the design of an inducible leaky oscillator. We further designed a circuit emulator, allowing the evaluation of our electrical designs on a large scale. We used the circuit emulator in conjunction with a robot simulator to demonstrate adaptive learning-based control of a robotic arm with six degrees of freedom.},
   author = {Avi Hazan and Elishai Ezra Tsur},
   doi = {10.3390/app12094528},
   issn = {20763417},
   issue = {9},
   journal = {Applied Sciences (Switzerland)},
   title = {Neuromorphic Neural Engineering Framework-Inspired Online Continuous Learning with Analog Circuitry},
   volume = {12},
   year = {2022},
}
@article{Vellaisamy2022,
   abstract = {Temporal Neural Networks (TNNs) are spiking neural networks that exhibit brain-like sensory processing with high energy efficiency. This work presents the ongoing research towards developing a custom design framework for designing efficient application-specific TNN-based Neuromorphic Sensory Processing Units (NSPUs). This paper examines previous works on NSPU designs for UCR time-series clustering and MNIST image classification applications. Current ideas for a custom design framework and tools that enable efficient software-to-hardware design flow for rapid design space exploration of application-specific NSPUs while leveraging EDA tools to obtain post-layout netlist and power-performance-area (PPA) metrics are described. Future research directions are also outlined.},
   author = {Prabhu Vellaisamy and John Paul Shen},
   month = {5},
   title = {Towards a Design Framework for TNN-Based Neuromorphic Sensory Processing Units},
   url = {http://arxiv.org/abs/2205.14248},
   year = {2022},
}
@inproceedings{Schuman2022,
   author = {Catherine Schuman and James Plank and Robert Patton and Thomas Potok and Garrett Rose},
   doi = {10.1145/3517343.3517370},
   booktitle = {ACM International Conference Proceeding Series},
   title = {A Framework to Enable Top-Down Co-Design of Neuromorphic Systems for Real-World Applications},
   year = {2022},
}
@inbook{Voelker2021,
   abstract = {As neuromorphic hardware begins to emerge as a viable target platform for artificial intelligence (AI) applications, there is a need for tools and software that can effectively compile a variety of AI models onto such hardware. Nengo (
            http://nengo.ai...},
   author = {Aaron R. Voelker and Chris Eliasmith},
   doi = {10.1007/978-981-15-2848-4_115-1},
   journal = {Handbook of Neuroengineering},
   title = {Programming Neuromorphics Using the Neural Engineering Framework},
   year = {2021},
}
@article{Pedersen2023,
   abstract = {Spiking neural networks and neuromorphic hardware platforms that emulate neural dynamics are slowly gaining momentum and entering main-stream usage. Despite a well-established mathematical foundation for neural dynamics, the implementation details vary greatly across different platforms. Correspondingly, there are a plethora of software and hardware implementations with their own unique technology stacks. Consequently, neuromorphic systems typically diverge from the expected computational model, which challenges the reproducibility and reliability across platforms. Additionally, most neuromorphic hardware is limited by its access via a single software frameworks with a limited set of training procedures. Here, we establish a common reference-frame for computations in neuromorphic systems, dubbed the Neuromorphic Intermediate Representation (NIR). NIR defines a set of computational primitives as idealized continuous-time hybrid systems that can be composed into graphs and mapped to and from various neuromorphic technology stacks. By abstracting away assumptions around discretization and hardware constraints, NIR faithfully captures the fundamental computation, while simultaneously exposing the exact differences between the evaluated implementation and the idealized mathematical formalism. We reproduce three NIR graphs across 7 neuromorphic simulators and 4 hardware platforms, demonstrating support for an unprecedented number of neuromorphic systems. With NIR, we decouple the evolution of neuromorphic hardware and software, ultimately increasing the interoperability between platforms and improving accessibility to neuromorphic technologies. We believe that NIR is an important step towards the continued study of brain-inspired hardware and bottom-up approaches aimed at an improved understanding of the computational underpinnings of nervous systems.},
   author = {Jens E. Pedersen and Steven Abreu and Matthias Jobst and Gregor Lenz and Vittorio Fra and Felix C. Bauer and Dylan R. Muir and Peng Zhou and Bernhard Vogginger and Kade Heckel and Gianvito Urgese and Sadasivan Shankar and Terrence C. Stewart and Jason K. Eshraghian and Sadique Sheik},
   month = {11},
   title = {Neuromorphic Intermediate Representation: A Unified Instruction Set for Interoperable Brain-Inspired Computing},
   url = {http://arxiv.org/abs/2311.14641},
   year = {2023},
}
@inproceedings{Date2022,
   abstract = {Neuromorphic computing is a non-von Neumann computing paradigm that performs computation by emulating the human brain. Neuromorphic systems are extremely energy-efficient and known to consume thousands of times less power than CPUs and GPUs. They have the potential to drive critical use cases such as autonomous vehicles, edge computing and internet of things in the future. For this reason, they are sought to be an indispensable part of the future computing landscape. Neuromorphic systems are mainly used for spike-based machine learning applications, although there are some non-machine learning applications in graph theory, differential equations, and spike-based simulations. These applications suggest that neuromorphic computing might be capable of general-purpose computing. However, general-purpose computability of neuromorphic computing has not been established yet. In this work, we prove that neuromorphic computing is Turing-complete and therefore capable of general-purpose computing. Specifically, we present a model of neuromorphic computing, with just two neuron parameters (threshold and leak), and two synaptic parameters (weight and delay). We devise neuromorphic circuits for computing all the μ-recursive functions (i.e., constant, successor and projection functions) and all the μ-recursive operators (i.e., composition, primitive recursion and minimization operators). Given that the μ-recursive functions and operators are precisely the ones that can be computed using a Turing machine, this work establishes the Turing-completeness of neuromorphic computing.},
   author = {Prasanna Date and Thomas Potok and Catherine Schuman and Bill Kay},
   doi = {10.1145/3546790.3546806},
   booktitle = {ACM International Conference Proceeding Series},
   title = {Neuromorphic Computing is Turing-Complete},
   year = {2022},
}
@misc{Chen2023,
   abstract = {The memristor is a resistive switch where its resistive state is programable based on the applied voltage or current. Memristive devices are thus capable of storing and computing information simultaneously, breaking the Von Neumann bottleneck. Since the first nanomemristor made by Hewlett-Packard in 2008, advances so far have enabled nanostructured, low-power, high-durability devices that exhibit superior performance over conventional CMOS devices. Herein, the development of memristors based on different physical mechanisms is reviewed. In particular, device stability, integration density, power consumption, switching speed, retention, and endurance of memristors, that are crucial for neuromorphic computing, are discussed in detail. An overview of various neural networks with a focus on building a memristor-based spike neural network neuromorphic computing system is then provided. Finally, the existing issues and challenges in implementing such neuromorphic computing systems are analyzed, and an outlook for brain-like computing is proposed.},
   author = {Wenbin Chen and Lekai Song and Shengbo Wang and Zhiyuan Zhang and Guanyu Wang and Guohua Hu and Shuo Gao},
   doi = {10.1002/aelm.202200833},
   issn = {2199160X},
   issue = {2},
   journal = {Advanced Electronic Materials},
   title = {Essential Characteristics of Memristors for Neuromorphic Computing},
   volume = {9},
   year = {2023},
}
@misc{Schuman2022,
   abstract = {Neuromorphic computing technologies will be important for the future of computing, but much of the work in neuromorphic computing has focused on hardware development. Here, we review recent results in neuromorphic computing algorithms and applications. We highlight characteristics of neuromorphic computing technologies that make them attractive for the future of computing and we discuss opportunities for future development of algorithms and applications on these systems.},
   author = {Catherine D. Schuman and Shruti R. Kulkarni and Maryam Parsa and J. Parker Mitchell and Prasanna Date and Bill Kay},
   doi = {10.1038/s43588-021-00184-y},
   issn = {26628457},
   issue = {1},
   journal = {Nature Computational Science},
   title = {Opportunities for neuromorphic computing algorithms and applications},
   volume = {2},
   year = {2022},
}
@inproceedings{Vaswani2017,
   abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
   author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Łukasz Kaiser and Illia Polosukhin},
   issn = {10495258},
   booktitle = {Advances in Neural Information Processing Systems},
   title = {Attention is all you need},
   volume = {2017-December},
   year = {2017},
}
